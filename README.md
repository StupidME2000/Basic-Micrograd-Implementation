Micrograd Learning - PyTorch and Custom MLP Neural Network

This repository contains an interactive Jupyter notebook that demonstrates the implementation of a simple multi-layer perceptron (MLP) using both PyTorch and a custom-built neural network inspired by the Micrograd library. The notebook covers:

Features
Gradient Calculation with PyTorch: Utilizes PyTorch to compute forward passes, gradients, and backpropagation for a simple neuron.
Custom MLP Implementation: Defines a multi-layer perceptron model from scratch using classes for neurons, layers, and the overall network.
Training Loop: Contains a basic training loop to minimize the loss by adjusting weights using gradient descent.
Data Examples: Provides training data and targets for demonstration of the MLP's learning process.

Topics Covered
Forward and backward propagation using PyTorch tensors
Custom implementation of a neural network, including:
Neuron
Layer
Multi-Layer Perceptron (MLP)
Gradient-based optimization for training

Dependencies
Python 3.7+
PyTorch
Jupyter Notebook
